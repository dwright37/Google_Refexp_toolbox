{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Hu et al Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "import operator\n",
    "from scipy import stats\n",
    "\n",
    "# MAKE Sure that google_refexp_py_lib is in your python libary search path\n",
    "# before you run API in this toolbox. You can use something as follows:\n",
    "sys.path.append('./google_refexp_py_lib')\n",
    "from refexp_eval import RefexpEvalComprehension\n",
    "from refexp_eval import RefexpEvalGeneration\n",
    "from common_utils import draw_bbox\n",
    "\n",
    "# Set coco_data_path and Google Refexp dataset validation set path\n",
    "coco_data_path = './external/coco/annotations/instances_train2014.json'\n",
    "refexp_dataset_path = './google_refexp_dataset_release/google_refexp_val_201511_coco_aligned.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google Refexp dataset file for the comprehension task.\n",
      "loading annotations into memory...\n",
      "Done (t=28.43s)\n",
      "creating index...\n",
      "index created!\n",
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Google Refexp dataset and MS COCO dataset (takes some time)\n",
    "eval_compreh = RefexpEvalComprehension(refexp_dataset_path, coco_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predicted result file for the comprehension task.\n",
      "Missing 9536 refexps in the refexp dataset file in the predicted file\n",
      "The average prec@1 score is 0.679\n"
     ]
    }
   ],
   "source": [
    "# We evalute a sample comprehension task results generated by\n",
    "# a naive method which outputs the groundtruth bounding boxes\n",
    "# in the coco image with a random order.\n",
    "pred_results_path = ('/home/dwright/repos/papers/Hu et al 2017/exp-refgoog/results/'\n",
    "                     'refgoog_attbilstm_iter_150000_val.txt')\n",
    "\n",
    "(prec, eval_results) = eval_compreh.evaluate(pred_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [(1, 1, 1), (1, 0, 0)]\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "        'text_cmap', colors)\n",
    "def makeHeatmap(attention, words, ax, title=\"\"):\n",
    "    plt.figure(figsize=(18, 1.5))\n",
    "    heatmap = plt.pcolor(attention, cmap=cm)\n",
    "    i = 0\n",
    "    for x in range(len(attention[0])):\n",
    "        plt.text(x + 0.5, 0.5, words[i], horizontalalignment='center', verticalalignment='center')\n",
    "        i += 1\n",
    "        \n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def multipage(filename, figs=None, dpi=200):\n",
    "    pp = PdfPages(filename)\n",
    "    if figs is None:\n",
    "        figs = [plt.figure(n) for n in plt.get_fignums()]\n",
    "    for fig in figs:\n",
    "        fig.savefig(pp, format='pdf')\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRefExpFromEval(sample):\n",
    "    ref = None\n",
    "    if 'refexp' in sample or 'refexp_id' in sample:\n",
    "        if 'refexp' in sample:\n",
    "            ref = sample['refexp']\n",
    "        else:\n",
    "            refexp_tmp = self.refexp_dataset.loadRefexps(ids=sample['refexp_id'])[0]\n",
    "            ref = refexp_tmp['raw']\n",
    "            \n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "def split(sentence):\n",
    "    if isinstance(sentence, bytes):\n",
    "        sentence = sentence.decode()\n",
    "    words = SENTENCE_SPLIT_REGEX.split(sentence.strip())\n",
    "    words = [w.lower() for w in words if len(w.strip()) > 0]\n",
    "    # remove .\n",
    "    if len(words) > 0 and (words[-1] == '.' or words[-1] == '?'):\n",
    "        words = words[:-1]\n",
    "        \n",
    "    if len(words) > 20:\n",
    "        words = words[:20]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printCategoryStats(results):\n",
    "    coco = eval_compreh.refexp_dataset.coco\n",
    "    cococats = coco.loadCats(coco.getCatIds())\n",
    "    cococats = {c['id'] : c for c in cococats}\n",
    "    anns = eval_compreh.refexp_dataset.loadAnns(ids=[r['annotation_id'] for r in results])\n",
    "    cats = {}\n",
    "    for ann in anns:\n",
    "        cat = cococats[ann['category_id']]['name']\n",
    "        if cat in cats:\n",
    "            cats[cat] += 1\n",
    "        else:\n",
    "            cats[cat] = 1\n",
    "\n",
    "    catIds = eval_compreh.refexp_dataset.getCatIds()\n",
    "    Get all images that contain all given categories.\n",
    "    totals = {}\n",
    "    for catId in catIds:\n",
    "        imgIds = eval_compreh.refexp_dataset.getImgIds(catIds=[catId])\n",
    "        totals[cococats[catId]['name']] = len(imgIds)\n",
    "\n",
    "    print('Missed bboxes by category')\n",
    "    for m in sorted(cats.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        print(str(m[0]) + \" -- missed: \" + str(m[1]) + \", total: \" + str(totals[m[0]]) + \", %: \" + str(float(m[1]) / totals[m[0]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative examples, generate statistics and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741 top 1 predicted bounding boxes have IoU with GT less than 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize the cases where your method performs bad/good. E.g.:\n",
    "bad_results = [res for res in eval_results if res['best_iou'] < 0.05]\n",
    "print ('%d top 1 predicted bounding boxes have IoU '\n",
    "       'with GT less than 0.1' % len(bad_results))\n",
    "print\n",
    "\n",
    "# Randomly sample 100 bad examples\n",
    "for i in range(100):\n",
    "    coco_image_dir = './external/coco/images/train2014'\n",
    "    bad_sample = bad_results[numpy.random.randint(0,len(bad_results))]\n",
    "    eval_compreh.visualize_top_predicted_bbox(bad_sample, coco_image_dir)\n",
    "     \n",
    "    ref = getRefExpFromEval(bad_sample)\n",
    "    ref = split(ref)\n",
    "    sub = [s[0] for s in bad_sample['obj1_prob']][-len(ref):]\n",
    "    makeHeatmap([sub], ref, plt.gca(), title='Subject attn')\n",
    "    rel = [s[0] for s in bad_sample['rel_prob']][-len(ref):]\n",
    "    makeHeatmap([rel], ref, plt.gca(), title='Relation attn')\n",
    "    obj = [s[0] for s in bad_sample['obj2_prob']][-len(ref):]\n",
    "    makeHeatmap([obj], ref, plt.gca(), title='Object attn')\n",
    "    multipage('/hdd/dustin/data/hu_et_al_eval/negative_samples/%.3d.pdf' % i)\n",
    "    #multipage('/hdd/dustin/data/hu_et_al_eval/negative_samples/test.pdf')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed bboxes by category\n",
      "person -- missed: 74, total: 108, %: 0.685185185185\n"
     ]
    }
   ],
   "source": [
    "printCategoryStats(bad_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "res_out = {\"results\": bad_results}\n",
    "with open('/home/dwright/repos/papers/Hu et al 2017/exp-refgoog/results/bad_results.json', 'w') as f:\n",
    "    json.dump(res_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/hdd/dustin/data/hu_et_al_eval/negative_samples/bad_refexps.txt', 'w') as f:\n",
    "    for sample in bad_results:\n",
    "        if 'refexp' in sample or 'refexp_id' in sample:\n",
    "            if 'refexp' in sample:\n",
    "                ref = sample['refexp']\n",
    "            else:\n",
    "                refexp_tmp = self.refexp_dataset.loadRefexps(ids=sample['refexp_id'])[0]\n",
    "                ref = refexp_tmp['raw']\n",
    "            f.write(ref + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 top 1 predicted bounding boxes have IoU with GT less than 0.1\n",
      "\n",
      "The Referring expression input to the model is:\n",
      "  the hand of someone holding scissors\n",
      "The Referring expression input to the model is:\n",
      "  There is a whole orange on the upper right side of the picture.\n",
      "The Referring expression input to the model is:\n",
      "  man skating in the right side of the image\n",
      "The Referring expression input to the model is:\n",
      "  girl in white top\n",
      "The Referring expression input to the model is:\n",
      "  the man in the right\n",
      "The Referring expression input to the model is:\n",
      "  the chair on the bottom right\n",
      "The Referring expression input to the model is:\n",
      "  A small white teddy bear sitting in front of a large bear.\n",
      "The Referring expression input to the model is:\n",
      "  A man in a black suit wearing safety glasses and a green tie.\n",
      "The Referring expression input to the model is:\n",
      "  The horse on the left.\n",
      "The Referring expression input to the model is:\n",
      "  cow in front\n",
      "The Referring expression input to the model is:\n",
      "  A SPARROW WITH TWO SPARROWS SITTING ON HANG ROD\n",
      "The Referring expression input to the model is:\n",
      "  left most glass\n",
      "The Referring expression input to the model is:\n",
      "  the chair on the bottom right\n",
      "The Referring expression input to the model is:\n",
      "  A WOMAN IN SKI GOOGLES HOLDING HER SKIS IN HER HANDS.\n",
      "The Referring expression input to the model is:\n",
      "  The cat on the right with toilet roll in front of him.\n",
      "The Referring expression input to the model is:\n",
      "  Man with patterned shirt\n",
      "The Referring expression input to the model is:\n",
      "  a woman posing with a tennis racket\n",
      "The Referring expression input to the model is:\n",
      "  A white bowl full of bean and tomato salsa.\n",
      "The Referring expression input to the model is:\n",
      "  A woman in a white shirt and black shorts playing in the sand.\n",
      "The Referring expression input to the model is:\n",
      "  zebra between two other zebras standing\n",
      "The Referring expression input to the model is:\n",
      "  The girl in the picture.\n",
      "The Referring expression input to the model is:\n",
      "  A hotdog between two other hotdogs.\n",
      "The Referring expression input to the model is:\n",
      "  Man in orange shirt on skateboard leaping up on the wave\n",
      "The Referring expression input to the model is:\n",
      "  Brunette man with beard and moustache wearing a maroon tie.\n",
      "The Referring expression input to the model is:\n",
      "  Giraffe to the right\n",
      "The Referring expression input to the model is:\n",
      "  A young man with a blue backpack.\n",
      "The Referring expression input to the model is:\n",
      "  A man in Black\n",
      "The Referring expression input to the model is:\n",
      "  Big brown cow with a tag in its ear\n",
      "The Referring expression input to the model is:\n",
      "  The bananas in the back .\n",
      "The Referring expression input to the model is:\n",
      "  An gloved hand of a chef who is cutting something\n",
      "The Referring expression input to the model is:\n",
      "  A partially seen person wearing gray shirt.\n",
      "The Referring expression input to the model is:\n",
      "  Big brown cow with a tag in its ear\n",
      "The Referring expression input to the model is:\n",
      "  giraffe short and on the right\n",
      "The Referring expression input to the model is:\n",
      "  A bowl of Wheat Thin crackers  on a table.\n",
      "The Referring expression input to the model is:\n",
      "  a piece of cake on a plate\n",
      "The Referring expression input to the model is:\n",
      "  a woman posing with a tennis racket\n",
      "The Referring expression input to the model is:\n",
      "  area of table with napkin holder,orange fanta, sunglasses, and pizza with no slices missing\n",
      "The Referring expression input to the model is:\n",
      "  Blue next to the girl feeding the giraffes.\n",
      "The Referring expression input to the model is:\n",
      "  a man in a white shirt\n",
      "The Referring expression input to the model is:\n",
      "  A person in a life vest bending down.\n",
      "The Referring expression input to the model is:\n",
      "  Big brown cow with a tag in its ear\n",
      "The Referring expression input to the model is:\n",
      "  A OLD MAN EATING SOMETHING AND TALKING WITH HIS LADY\n",
      "The Referring expression input to the model is:\n",
      "  a boy in blue jersey number 26 is playing with baseball bat.\n",
      "The Referring expression input to the model is:\n",
      "  a black umbrella covering two women\n",
      "The Referring expression input to the model is:\n",
      "  An all white bird all the way on the left.\n",
      "The Referring expression input to the model is:\n",
      "  The half of a sanwhich that is on the left of the plate without a bite out of it on the left part of the table.\n",
      "The Referring expression input to the model is:\n",
      "  A boy standing.\n",
      "The Referring expression input to the model is:\n",
      "  A woman wearing a plaid shirt and gray vest standing next to a horse\n",
      "The Referring expression input to the model is:\n",
      "  a bird with its beak open\n",
      "The Referring expression input to the model is:\n",
      "  RED COLOR BUS STANDING THE ROAD IN THE SECOND POSITION\n",
      "The Referring expression input to the model is:\n",
      "  A catcher watching as a batter hits the ball.\n",
      "The Referring expression input to the model is:\n",
      "  Forearms and hands of a person wearing a silver watch that is preparing a sandwich.\n",
      "The Referring expression input to the model is:\n",
      "  The green granite table top\n",
      "The Referring expression input to the model is:\n",
      "  A kid kneeling down in front of a dog.\n",
      "The Referring expression input to the model is:\n",
      "  The man in the long sleeved light shirt.\n",
      "The Referring expression input to the model is:\n",
      "  A bowl with soup.\n",
      "The Referring expression input to the model is:\n",
      "  A woman holding a tennis racquet.\n",
      "The Referring expression input to the model is:\n",
      "  cat touching toilet paper roll\n",
      "The Referring expression input to the model is:\n",
      "  beauty queen wearing pink\n",
      "The Referring expression input to the model is:\n",
      "  The shaded area to the bottom left of the plate.\n",
      "The Referring expression input to the model is:\n",
      "  An elephant to the left of two other elephants.\n",
      "The Referring expression input to the model is:\n",
      "  A narrow vase among all\n",
      "The Referring expression input to the model is:\n",
      "  A bored looking man wearing a blanket, holding a cat, and browsing the internet.\n",
      "The Referring expression input to the model is:\n",
      "  A flower vase that is very narrow\n",
      "The Referring expression input to the model is:\n",
      "  The left hand horse of a pair of horses pulling a wagon reacting to a gunshot.\n",
      "The Referring expression input to the model is:\n",
      "  sandwich in the front of the image\n",
      "The Referring expression input to the model is:\n",
      "  The green granite table top\n",
      "The Referring expression input to the model is:\n",
      "  An umpire calling the game.\n",
      "The Referring expression input to the model is:\n",
      "  Someone's hand is holding a bottle of water.\n",
      "The Referring expression input to the model is:\n",
      "  The maroon office chair behind the fan\n",
      "The Referring expression input to the model is:\n",
      "  The back of the bus with 'Sai' on it\n",
      "The Referring expression input to the model is:\n",
      "  a bunch of green bananas\n",
      "The Referring expression input to the model is:\n",
      "  skater picture to the left\n",
      "The Referring expression input to the model is:\n",
      "  A man with a dark jacket\n",
      "The Referring expression input to the model is:\n",
      "  The buffalo standing between two other buffalo\n",
      "The Referring expression input to the model is:\n",
      "  The rear flank of a zebra standing near other zebras.\n",
      "The Referring expression input to the model is:\n",
      "  a reflection of a suv\n",
      "The Referring expression input to the model is:\n",
      "  The boy on the far left with short, curly, blond hair.\n",
      "The Referring expression input to the model is:\n",
      "  Little girl in pink.\n",
      "The Referring expression input to the model is:\n",
      "  A black leather chair with a gold pillow.\n",
      "The Referring expression input to the model is:\n",
      "  Brown horse with a blanket on its back on the right tied next to other horses at a hitching post\n",
      "The Referring expression input to the model is:\n",
      "  person on right\n",
      "The Referring expression input to the model is:\n",
      "  a whole no topping pizza\n",
      "The Referring expression input to the model is:\n",
      "  Zebra behind three other zebras.\n",
      "The Referring expression input to the model is:\n",
      "  Doughnut in top left corner\n",
      "The Referring expression input to the model is:\n",
      "  A piece of some food, behind the other piece.\n",
      "The Referring expression input to the model is:\n",
      "  The tennis racket that the man in the white shirt and black shorts is holding.\n",
      "The Referring expression input to the model is:\n",
      "  A man in a striped polo shirt looking towards the ground\n",
      "The Referring expression input to the model is:\n",
      "  A television.\n",
      "The Referring expression input to the model is:\n",
      "  zebra walking to back side in the right side of the image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Referring expression input to the model is:\n",
      "  A woman in a green jacket with skis and poles.\n",
      "The Referring expression input to the model is:\n",
      "  Catcher behind the batter\n",
      "The Referring expression input to the model is:\n",
      "  The giraffe whose head is not visible.\n",
      "The Referring expression input to the model is:\n",
      "  Wine glass directly infront of white cheese with blue streak.\n",
      "The Referring expression input to the model is:\n",
      "  Duck being hit with other duck's wing.\n",
      "The Referring expression input to the model is:\n",
      "  hat in the right most side of the image\n",
      "The Referring expression input to the model is:\n",
      "  the man holding a beer\n",
      "The Referring expression input to the model is:\n",
      "  White guy with mole on left side of face.\n",
      "The Referring expression input to the model is:\n",
      "  front most hot pocket piece\n",
      "The Referring expression input to the model is:\n",
      "  a black leather dinning chair\n"
     ]
    }
   ],
   "source": [
    "# Visualize the cases where your method performs bad/good. E.g.:\n",
    "good_results = [res for res in eval_results if res['best_iou'] >= 0.5]\n",
    "print ('%d top 1 predicted bounding boxes have IoU '\n",
    "       'with GT less than 0.1' % len(good_results))\n",
    "print\n",
    "\n",
    "# Randomly sample 100 bad examples\n",
    "for i in range(100):\n",
    "    coco_image_dir = './external/coco/images/train2014'\n",
    "    good_sample = good_results[numpy.random.randint(0,len(good_results))]\n",
    "    eval_compreh.visualize_top_predicted_bbox(good_sample, coco_image_dir)\n",
    "     \n",
    "    ref = getRefExpFromEval(good_sample)\n",
    "    ref = split(ref)\n",
    "    sub = [s[0] for s in good_sample['obj1_prob']][-len(ref):]\n",
    "    makeHeatmap([sub], ref, plt.gca(), title='Subject attn')\n",
    "    rel = [s[0] for s in good_sample['rel_prob']][-len(ref):]\n",
    "    makeHeatmap([rel], ref, plt.gca(), title='Relation attn')\n",
    "    obj = [s[0] for s in good_sample['obj2_prob']][-len(ref):]\n",
    "    makeHeatmap([obj], ref, plt.gca(), title='Object attn')\n",
    "    multipage('/hdd/dustin/data/hu_et_al_eval/gref_multiobject_positive_samples/%.3d.pdf' % i)\n",
    "    #multipage('/hdd/dustin/data/hu_et_al_eval/negative_samples/test.pdf')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6166281755196305"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_results) / float(len(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missed bboxes by category\n",
      "person -- missed: 1486, total: 2936, %: 0.506130790191\n",
      "chair -- missed: 178, total: 870, %: 0.204597701149\n",
      "giraffe -- missed: 135, total: 166, %: 0.813253012048\n",
      "dining table -- missed: 102, total: 929, %: 0.10979547901\n",
      "zebra -- missed: 101, total: 128, %: 0.7890625\n",
      "couch -- missed: 96, total: 380, %: 0.252631578947\n",
      "bowl -- missed: 94, total: 478, %: 0.196652719665\n",
      "car -- missed: 90, total: 441, %: 0.204081632653\n",
      "elephant -- missed: 79, total: 113, %: 0.699115044248\n",
      "horse -- missed: 64, total: 129, %: 0.496124031008\n",
      "truck -- missed: 61, total: 240, %: 0.254166666667\n",
      "cow -- missed: 60, total: 86, %: 0.697674418605\n",
      "bus -- missed: 59, total: 134, %: 0.440298507463\n",
      "cup -- missed: 54, total: 652, %: 0.0828220858896\n",
      "sandwich -- missed: 53, total: 204, %: 0.259803921569\n",
      "pizza -- missed: 52, total: 219, %: 0.237442922374\n",
      "motorcycle -- missed: 51, total: 160, %: 0.31875\n",
      "airplane -- missed: 46, total: 63, %: 0.730158730159\n",
      "cat -- missed: 45, total: 113, %: 0.398230088496\n",
      "train -- missed: 40, total: 69, %: 0.579710144928\n",
      "broccoli -- missed: 38, total: 111, %: 0.342342342342\n",
      "dog -- missed: 38, total: 156, %: 0.24358974359\n",
      "sheep -- missed: 37, total: 63, %: 0.587301587302\n",
      "vase -- missed: 37, total: 181, %: 0.204419889503\n",
      "potted plant -- missed: 37, total: 262, %: 0.141221374046\n",
      "laptop -- missed: 37, total: 188, %: 0.196808510638\n",
      "cake -- missed: 37, total: 193, %: 0.19170984456\n",
      "bench -- missed: 36, total: 206, %: 0.174757281553\n",
      "tv -- missed: 34, total: 250, %: 0.136\n",
      "boat -- missed: 33, total: 69, %: 0.478260869565\n",
      "bird -- missed: 33, total: 87, %: 0.379310344828\n",
      "bicycle -- missed: 32, total: 133, %: 0.240601503759\n",
      "teddy bear -- missed: 29, total: 93, %: 0.311827956989\n",
      "banana -- missed: 29, total: 129, %: 0.22480620155\n",
      "umbrella -- missed: 29, total: 147, %: 0.197278911565\n",
      "bed -- missed: 28, total: 123, %: 0.227642276423\n",
      "suitcase -- missed: 27, total: 107, %: 0.252336448598\n",
      "apple -- missed: 25, total: 108, %: 0.231481481481\n",
      "carrot -- missed: 24, total: 119, %: 0.201680672269\n",
      "donut -- missed: 23, total: 95, %: 0.242105263158\n",
      "book -- missed: 20, total: 276, %: 0.0724637681159\n",
      "bear -- missed: 19, total: 31, %: 0.612903225806\n",
      "skis -- missed: 18, total: 100, %: 0.18\n",
      "wine glass -- missed: 18, total: 205, %: 0.0878048780488\n",
      "keyboard -- missed: 17, total: 99, %: 0.171717171717\n",
      "backpack -- missed: 17, total: 247, %: 0.0688259109312\n",
      "bottle -- missed: 17, total: 500, %: 0.034\n",
      "orange -- missed: 16, total: 96, %: 0.166666666667\n",
      "fork -- missed: 15, total: 305, %: 0.0491803278689\n",
      "surfboard -- missed: 15, total: 58, %: 0.258620689655\n",
      "sink -- missed: 14, total: 93, %: 0.150537634409\n",
      "handbag -- missed: 13, total: 322, %: 0.0403726708075\n",
      "skateboard -- missed: 12, total: 111, %: 0.108108108108\n",
      "knife -- missed: 12, total: 330, %: 0.0363636363636\n",
      "hot dog -- missed: 12, total: 79, %: 0.151898734177\n",
      "clock -- missed: 12, total: 118, %: 0.101694915254\n",
      "traffic light -- missed: 12, total: 83, %: 0.144578313253\n",
      "oven -- missed: 11, total: 96, %: 0.114583333333\n",
      "toilet -- missed: 10, total: 33, %: 0.30303030303\n",
      "snowboard -- missed: 10, total: 54, %: 0.185185185185\n",
      "kite -- missed: 10, total: 38, %: 0.263157894737\n",
      "tennis racket -- missed: 9, total: 113, %: 0.0796460176991\n",
      "tie -- missed: 7, total: 197, %: 0.0355329949239\n",
      "parking meter -- missed: 7, total: 28, %: 0.25\n",
      "cell phone -- missed: 7, total: 277, %: 0.0252707581227\n",
      "scissors -- missed: 5, total: 38, %: 0.131578947368\n",
      "spoon -- missed: 5, total: 247, %: 0.0202429149798\n",
      "toothbrush -- missed: 4, total: 47, %: 0.0851063829787\n",
      "refrigerator -- missed: 4, total: 70, %: 0.0571428571429\n",
      "fire hydrant -- missed: 3, total: 34, %: 0.0882352941176\n",
      "remote -- missed: 3, total: 268, %: 0.0111940298507\n",
      "microwave -- missed: 2, total: 48, %: 0.0416666666667\n",
      "stop sign -- missed: 2, total: 23, %: 0.0869565217391\n",
      "baseball bat -- missed: 2, total: 106, %: 0.0188679245283\n",
      "sports ball -- missed: 1, total: 144, %: 0.00694444444444\n",
      "mouse -- missed: 1, total: 87, %: 0.0114942528736\n"
     ]
    }
   ],
   "source": [
    "printCategoryStats(good_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "res_out = {\"results\": good_results}\n",
    "with open('/home/dwright/repos/papers/Hu et al 2017/exp-refgoog/results/good_results.json', 'w') as f:\n",
    "    json.dump(res_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/hdd/dustin/data/hu_et_al_eval/positive_samples/good_refexps.txt', 'w') as f:\n",
    "    for sample in good_results:\n",
    "        ref = getRefExpFromEval(sample)\n",
    "        f.write(ref + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate KL distance for attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avg K(s||r): ', 0.48298778563366879)\n",
      "('Avg K(r||s): ', 0.52495721460993305)\n",
      "('Avg K(s||o): ', 0.41107716988187898)\n",
      "('Avg K(o||s): ', 0.48697174823757983)\n",
      "('Avg K(r||o): ', 0.015733561862567511)\n",
      "('Avg K(o||r): ', 0.015772716419188097)\n"
     ]
    }
   ],
   "source": [
    "sr_kl = []\n",
    "rs_kl = []\n",
    "so_kl = []\n",
    "os_kl = []\n",
    "ro_kl = []\n",
    "or_kl = []\n",
    "\n",
    "for res in eval_results:\n",
    "    ref = getRefExpFromEval(res)\n",
    "    ref = split(ref)\n",
    "    sub = [s[0] for s in res['obj1_prob']][-len(ref):]\n",
    "    rel = [s[0] for s in res['rel_prob']][-len(ref):]\n",
    "    obj = [s[0] for s in res['obj2_prob']][-len(ref):]\n",
    "    \n",
    "    sr_kl.append(stats.entropy(sub, rel))\n",
    "    rs_kl.append(stats.entropy(rel, sub))\n",
    "    \n",
    "    so_kl.append(stats.entropy(sub, obj))\n",
    "    os_kl.append(stats.entropy(obj, sub))\n",
    "    \n",
    "    ro_kl.append(stats.entropy(rel, obj))\n",
    "    or_kl.append(stats.entropy(obj, rel))\n",
    "    \n",
    "print('Avg K(s||r): ', numpy.mean(sr_kl))\n",
    "print('Avg K(r||s): ', numpy.mean(rs_kl))\n",
    "\n",
    "print('Avg K(s||o): ', numpy.mean(so_kl))\n",
    "print('Avg K(o||s): ', numpy.mean(os_kl))\n",
    "\n",
    "print('Avg K(r||o): ', numpy.mean(ro_kl))\n",
    "print('Avg K(o||r): ', numpy.mean(or_kl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def getWeightedEmbedding(tokens, weights, wv, unk):\n",
    "    return numpy.sum([weights[i] * wv[tokens[i]] if tokens[i] in wv.vocab else weights[i] * unk for i in range(len(tokens))], axis=0)\n",
    "\n",
    "#Get word vectors\n",
    "wv = KeyedVectors.load_word2vec_format('/hdd/dustin/data/word_vectors/glove.6B.100d.w2vformat.txt', binary=False)\n",
    "unk = numpy.random.normal(loc=0., scale=1., size=(wv.vector_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "#Calculate avg embedding using attn\n",
    "for res in eval_results:\n",
    "    ref = getRefExpFromEval(res)\n",
    "    ref = split(ref)\n",
    "    sub = [s[0] for s in res['obj1_prob']][-len(ref):]\n",
    "    rel = [s[0] for s in res['rel_prob']][-len(ref):]\n",
    "    obj = [s[0] for s in res['obj2_prob']][-len(ref):]\n",
    "    \n",
    "    embeddings.append(getWeightedEmbedding(ref, sub, wv, unk))\n",
    "    embeddings.append(getWeightedEmbedding(ref, rel, wv, unk))\n",
    "    embeddings.append(getWeightedEmbedding(ref, obj, wv, unk))\n",
    "\n",
    "embeddings = numpy.asarray(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA to 2 components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "#Plot\n",
    "def plot(data, labs=[], colors=[], handles=None, title=\"Plot of data after PCA\", pointSize=0.5):\n",
    "    print(len(data))\n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(data[:,0], data[:,1], s=pointSize)\n",
    "    ax.set_title(title)\n",
    "    for i in range(len(data)):\n",
    "        ax.annotate(labs[i], (data[i,0],data[i,1]), fontsize=14, color=colors[i])\n",
    "        \n",
    "    if handles is not None:\n",
    "        ax.legend(handles=handles)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "samples = numpy.asarray(eval_results)[numpy.random.randint(len(eval_results), size=20)]\n",
    "\n",
    "ems_sub = []\n",
    "ems_obj = []\n",
    "ems_rel = []\n",
    "labs = []\n",
    "for sam in samples:\n",
    "    ref_orig = getRefExpFromEval(sam)\n",
    "    ref = split(ref_orig)\n",
    "    sub = [s[0] for s in res['obj1_prob']][-len(ref):]\n",
    "    rel = [s[0] for s in res['rel_prob']][-len(ref):]\n",
    "    obj = [s[0] for s in res['obj2_prob']][-len(ref):]\n",
    "\n",
    "    ems_sub.append(getWeightedEmbedding(ref, sub, wv, unk))\n",
    "    ems_rel.append(getWeightedEmbedding(ref, rel, wv, unk))\n",
    "    ems_obj.append(getWeightedEmbedding(ref, obj, wv, unk))\n",
    "    labs.append(ref_orig)\n",
    "\n",
    "spoints = pca.transform(ems_sub)\n",
    "rpoints = pca.transform(ems_rel)\n",
    "opoints = pca.transform(ems_obj)\n",
    "\n",
    "scolors = ['red'] * len(spoints)\n",
    "rcolors = ['blue'] * len(rpoints)\n",
    "ocolors = ['green'] * len(opoints)\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='Subject attention')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Relation attention')\n",
    "green_patch = mpatches.Patch(color='green', label='Object attention')\n",
    "\n",
    "fig = plot(numpy.concatenate([spoints, rpoints]), labs + labs, scolors + rcolors, title='2D projection of BOW with subject and relation attention', handles=[red_patch, blue_patch])\n",
    "fig.savefig('/hdd/dustin/data/hu_et_al_eval/sub_rel_attention.pdf')\n",
    "plt.close('all')\n",
    "\n",
    "fig = plot(numpy.concatenate([rpoints, opoints]), labs + labs, rcolors + ocolors, title='2D projection of BOW with relation and object attention', handles=[blue_patch, green_patch])\n",
    "fig.savefig('/hdd/dustin/data/hu_et_al_eval/rel_obj_attention.pdf')\n",
    "plt.close('all')\n",
    "\n",
    "fig = plot(numpy.concatenate([spoints, opoints]), labs + labs, scolors + ocolors, title='2D projection of BOW with subject and object attention', handles=[red_patch, green_patch])\n",
    "fig.savefig('/hdd/dustin/data/hu_et_al_eval/sub_obj_attention.pdf')\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
